# Medical Transformer: Gated Axial-Attention forMedical Image Segmentation
## Abstract
1. Lack of understanding of long-rand dependencies in the images due to _inductive bias_.





## Good experessions:
1. Leverage (利用): Recently proposed transformer-basedarchitectures that leverage self-attention mechanism encode long-rangedependencies and learn representations that are highly expressive.
2. Fesibility (可行性): the fesibility of ...
3. To this end: 为此
4. 
